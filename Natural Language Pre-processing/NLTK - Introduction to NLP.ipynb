{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e803c04",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c413a4e",
   "metadata": {},
   "source": [
    "### Install NLTK\n",
    "```pip install nltk```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f68b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "046c218c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2a22b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus- A large collection of text\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1a6d90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89ff6a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "975dac2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let say i want sentences from adventure category\n",
    "data = brown.sents(categories='adventure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75f67f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.'], ['He', 'was', 'well', 'rid', 'of', 'her', '.'], ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29284be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4637"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data) #number of sentences present in adventure cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b660cb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dan', 'Morgan', 'told', 'himself', 'he', 'would', 'forget', 'Ann', 'Turner', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "277605d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He was well rid of her .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(data[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809270fd",
   "metadata": {},
   "source": [
    "# Bag of Words Pipeline\n",
    "- Get the Data/Corpus\n",
    "- Tokenisation(Break doc in sentences, and further sentences into words), \n",
    "- Stopward Removal\n",
    "- Stemming\n",
    "- Building a Vocab\n",
    "- Vectorization\n",
    "- Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d13cacb",
   "metadata": {},
   "source": [
    "### Tokenisation @ Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1f2980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"\"\"It was a very pleasant day. The weather was cool and there were light showers.\n",
    "I went to the market to but some fruits.\"\"\"\n",
    "\n",
    "sentence = \"Send all the documents related to chapters 1,2,3 at vivekkrrai.bxr@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd910499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c734ea35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It was a very pleasant day.', 'The weather was cool and there were light showers.', 'I went to the market to but some fruits.']\n"
     ]
    }
   ],
   "source": [
    "sents = sent_tokenize(document) #break a document into list of sentences\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9779f324",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74bc8af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'It was a very pleasant day.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257e6ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'vivekkrrai.bxr@gmail.com']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9445dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# .split() cannot split 1,2,3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d201eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Send', 'all', 'the', 'documents', 'related', 'to', 'chapters', '1,2,3', 'at', 'vivekkrrai.bxr', '@', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "#now let us use word tokeniser\n",
    "words = word_tokenize(sentence)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca44600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can also see that it breaks about special char"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f323513",
   "metadata": {},
   "source": [
    "### Stopword removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e22c0f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db0bfff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06fbf2e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'themselves', 'these', 'itself', 'further', 'been', 'only', 'but', 'all', 'too', 'do', \"needn't\", 'her', 'over', \"it's\", \"should've\", 'was', 'yourselves', 'from', 'herself', 'out', 'for', 'in', 'there', 'each', \"doesn't\", 'yours', 'needn', 'if', \"you've\", 'my', 'will', 'me', 'we', 'during', 's', 'nor', 'above', 'himself', 'no', 'of', 'being', 'to', 'under', \"wouldn't\", 'it', 'below', 'both', \"you're\", 'down', 'while', 'more', 'won', 'into', 'up', 'them', 'just', 'didn', 'll', 'were', 'this', 'don', 'ma', 'theirs', 'him', 'ourselves', 'that', 'm', 'ain', 'between', 'through', 'again', 'ours', 'its', 'had', 'does', 'whom', 'because', 'here', 'as', 'than', \"mightn't\", \"hasn't\", 'be', 'has', 'his', 'can', 'd', 'how', 'doing', 'myself', \"couldn't\", 're', 'against', \"aren't\", 'the', \"won't\", 'where', 'their', 'off', \"weren't\", 'by', 't', 'mustn', 'she', 'at', 'now', 'aren', 'and', 'on', 'weren', \"you'll\", 'very', 'who', 'shan', \"mustn't\", 'an', \"that'll\", 'a', \"didn't\", 'not', 'why', 'he', 'your', 'what', 'other', 'our', 'most', 'are', 'doesn', 'or', 'then', 'same', \"don't\", 'have', 'hers', 'y', 'is', 'mightn', \"shouldn't\", 'having', 'couldn', 'before', 'once', 'those', 'hadn', 'wasn', 'shouldn', 'yourself', 'when', 'own', \"hadn't\", 'o', 'hasn', \"wasn't\", 'did', 'with', 've', 'am', \"you'd\", 'about', 'such', 'wouldn', 'so', 'until', \"haven't\", 'they', \"isn't\", \"shan't\", 'few', 'you', 'after', 'isn', 'which', \"she's\", 'i', 'some', 'any', 'haven', 'should'}\n"
     ]
    }
   ],
   "source": [
    "print(sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9f7a78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#these are the words that can be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "678c45ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stopwords):\n",
    "    useful_words = [w for w in text if w not in stopwords]\n",
    "    return useful_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1bb86d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'bothered', 'much.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am not bothered about her very much.\".split()\n",
    "useful_text = remove_stopwords(text, sw)\n",
    "print(useful_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "440a73a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'not' in sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81773968",
   "metadata": {},
   "source": [
    "### Tokenisation using Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e50caa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Send all the 50 documents related to chapters 1,2,3 at vivekkrrai.bxr@gmail.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aee2c4ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " '50',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " '1,2,3',\n",
       " 'at',\n",
       " 'vivekkrrai.bxr@gmail.com']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "affb41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62bdbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer('[a-zA-Z@.]+') #include all letters, @,., + sign means for words\n",
    "useful_text = tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03462946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Send',\n",
       " 'all',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'related',\n",
       " 'to',\n",
       " 'chapters',\n",
       " 'at',\n",
       " 'vivekkrrai.bxr@gmail.com']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_text #ALl numbers are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178ba593",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Process that transforms words(verbs,plurals) into their radical form\n",
    "- Preserve the semantics of the sentence wothout increasing the number of unique tokens\n",
    "- Example - jumps, jumping, jumped, jump ==> jump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c6173bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Foxes love to make jumps. The quick brown fox was seen jumping over the\n",
    "lovely dog from a 6ft high wall\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95a5291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Snowball Stemmer, Porter, LAnchaster Stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer, PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "39bc72c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a3fac328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumpping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f817d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('jumps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ec50c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b7aff298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2a7e9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#it removes -s,-ing,-ly- etc at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a743c0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snowball Stemmer - It is a multilingual Stemmebr works in many languages\n",
    "ss = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8f66ef0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'love'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('lovely')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0cfa3606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jump'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.stem('jumping')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a20f0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jumping'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Lemmatization\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wn = WordNetLemmatizer()\n",
    "wn.lemmatize('jumping')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b0bb2f",
   "metadata": {},
   "source": [
    "## Building a Vocab and Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f38efcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample corpus contains 4 documents, each document can have 1 or more sentences\n",
    "\n",
    "corpus = [\n",
    "        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n",
    "        'We will win next Lok Sabha Elections, says confident Indian PM',\n",
    "        'The nobel laurate won the hearts of the people.',\n",
    "        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "87c86bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e600358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b2ac725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5aa7a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = vectorized_corpus.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c884c14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "[0 1 0 1 1 0 1 2 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0\n",
      " 2 0 1 0 2]\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))\n",
    "print(vectorized_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4e3d38ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indian': 12,\n",
       " 'cricket': 6,\n",
       " 'team': 31,\n",
       " 'will': 37,\n",
       " 'wins': 39,\n",
       " 'world': 41,\n",
       " 'cup': 7,\n",
       " 'says': 27,\n",
       " 'capt': 4,\n",
       " 'virat': 35,\n",
       " 'kohli': 14,\n",
       " 'be': 3,\n",
       " 'held': 11,\n",
       " 'at': 1,\n",
       " 'sri': 29,\n",
       " 'lanka': 15,\n",
       " 'we': 36,\n",
       " 'win': 38,\n",
       " 'next': 19,\n",
       " 'lok': 17,\n",
       " 'sabha': 26,\n",
       " 'elections': 8,\n",
       " 'confident': 5,\n",
       " 'pm': 23,\n",
       " 'the': 32,\n",
       " 'nobel': 20,\n",
       " 'laurate': 16,\n",
       " 'won': 40,\n",
       " 'hearts': 10,\n",
       " 'of': 21,\n",
       " 'people': 22,\n",
       " 'movie': 18,\n",
       " 'raazi': 24,\n",
       " 'is': 13,\n",
       " 'an': 0,\n",
       " 'exciting': 9,\n",
       " 'spy': 28,\n",
       " 'thriller': 33,\n",
       " 'based': 2,\n",
       " 'upon': 34,\n",
       " 'real': 25,\n",
       " 'story': 30}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_ #a/q to this cricket word has got the 6th index and that is why\n",
    "#and sixth index we have 1 in vectorised_corpus[0] correspnding to 1st doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "abfcdc18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32487f83",
   "metadata": {},
   "source": [
    "we can see the vectorised_corpus has same number of elements as there are \n",
    "key value pairs. Each position shows the frequency of elemnts and if an elemnt from the entire data is\n",
    "not present in the vector of 1st doc it is 0.\n",
    "suppose cup has frequency 2 cuz it occurs 2 times in 1st doc and 'an' has freq 0 cuz it doesn't occur in 1st doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f2a04cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to convert this dict mapping back to the sentence\n",
    "#reverse mapping\n",
    "numbers = vectorized_corpus[2]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4a35ff55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['hearts', 'laurate', 'nobel', 'of', 'people', 'the', 'won'],\n",
       "       dtype='<U9')]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s= cv.inverse_transform(numbers)\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02149d5b",
   "metadata": {},
   "source": [
    "## Vectorisation with Stopward removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aa2598ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myTokenizer(document):\n",
    "    words = tokenizer.tokenize(document.lower()) #convert every thing in lower case\n",
    "    #Remove Stopwords\n",
    "    words = remove_stopwords(words, sw)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8287c045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['send', 'documents', 'related', 'chapters', 'vivekkrrai.bxr@gmail.com']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myTokenizer(sentence)\n",
    "#print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fd9125d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer = myTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f74e7a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_corpus = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2e940ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 1 2 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 1 0 1 2]\n",
      " [0 0 1 0 0 1 0 0 0 1 0 0 0 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 1 0 0 1 0 1 0 1 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "660843fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorized_corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1723e361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['capt.', 'cricket', 'cup', 'held', 'indian', 'kohli.', 'lanka.',\n",
       "        'says', 'sri', 'team', 'virat', 'wins', 'world'], dtype='<U9'),\n",
       " array(['confident', 'elections', 'indian', 'lok', 'next', 'pm', 'sabha',\n",
       "        'says', 'win'], dtype='<U9'),\n",
       " array(['hearts', 'laurate', 'nobel', 'people.'], dtype='<U9'),\n",
       " array(['based', 'exciting', 'indian', 'movie', 'raazi', 'real', 'spy',\n",
       "        'story.', 'thriller', 'upon'], dtype='<U9')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.inverse_transform(vectorized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2c4d38",
   "metadata": {},
   "source": [
    "We can see that not all words are present. Some of them were removed during stopword removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "aed87500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For test data\n",
    "\n",
    "test_corpus = [\n",
    "    'Indian cricket rocks!',\n",
    "]\n",
    "#we will not call fit transform for test data as it will shrink the vocab and change it.\n",
    "cv.transform(test_corpus).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f7e518",
   "metadata": {},
   "source": [
    "So for Train data we call ```fit_transform()``` but for test data we only call ```transform()```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775af96a",
   "metadata": {},
   "source": [
    "## More ways to create Features\n",
    "- Unigram - every word as a feature\n",
    "- Bigrams - two consecutive words can be treated as a single feature\n",
    "- Trigrams - three consecutive ....\n",
    "- n-grams - and so on\n",
    "- TF-IDF Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "0cf141d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = [\"this is good movie\"]\n",
    "# sent_2 = [\"this is good move but actor is not present\"]\n",
    "sent_3 = [\"this is not good movie\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7a13093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69360d14",
   "metadata": {},
   "source": [
    "```ngram_range=(1,1)``` is for Unigram, ```ngram_range=(2,2)``` is for Bigrams\n",
    "```ngram_range=(3,3)``` is for Trigrams and ```ngram_range=(1,3)``` means combinations of Unigram, Bigram and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "76aafc43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0],\n",
       "       [1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [sent_1[0], sent_3[0]]\n",
    "cv.fit_transform(docs).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ad2f2962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 11,\n",
       " 'is': 2,\n",
       " 'good': 0,\n",
       " 'movie': 7,\n",
       " 'this is': 12,\n",
       " 'is good': 3,\n",
       " 'good movie': 1,\n",
       " 'this is good': 13,\n",
       " 'is good movie': 4,\n",
       " 'not': 8,\n",
       " 'is not': 5,\n",
       " 'not good': 9,\n",
       " 'this is not': 14,\n",
       " 'is not good': 6,\n",
       " 'not good movie': 10}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34a142a",
   "metadata": {},
   "source": [
    "### Tf-idf Normalisation\n",
    "- Avoid features that occur very often, becauase they contain less information\n",
    "- Information decreases as the number of occurences increases across different type of documents\n",
    "- So we define another term - term-document-frequency which associates a weight with every term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3d283ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sent_1  = \"this is good movie\"\n",
    "sent_2 = \"this was good movie\"\n",
    "sent_3 = \"this is not good movie\"\n",
    "\n",
    "corpus = [sent_1,sent_2,sent_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "2b3726b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1760c9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "50d5d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c305cb59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 2)\t0.4633342717458061\n",
      "  (0, 0)\t0.4633342717458061\n",
      "  (0, 1)\t0.5966272352795762\n",
      "  (0, 4)\t0.4633342717458061\n",
      "  (1, 5)\t0.6990303272568005\n",
      "  (1, 2)\t0.4128585720620119\n",
      "  (1, 0)\t0.4128585720620119\n",
      "  (1, 4)\t0.4128585720620119\n",
      "  (2, 3)\t0.6172273175654565\n",
      "  (2, 2)\t0.3645443967613799\n",
      "  (2, 0)\t0.3645443967613799\n",
      "  (2, 1)\t0.4694172843223779\n",
      "  (2, 4)\t0.3645443967613799\n"
     ]
    }
   ],
   "source": [
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0626f849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.46333427 0.59662724 0.46333427 0.         0.46333427 0.        ]\n",
      " [0.41285857 0.         0.41285857 0.         0.41285857 0.69903033]\n",
      " [0.3645444  0.46941728 0.3645444  0.61722732 0.3645444  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "vc = vc.toarray()\n",
    "print(vc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "02cfa46e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 4, 'is': 1, 'good': 0, 'movie': 2, 'was': 5, 'not': 3}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93aeaa55",
   "metadata": {},
   "source": [
    "- We can clearly see that 'not' has highest frequency than other words cuz it appears in only one document. \n",
    "- Word 'is' will have more if-idf value than this, good cuz it occurs 2 times in while doc and this, good appear 3 times.\n",
    "- Basically, the words which appear more frequently will be given less weightage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd35d0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
